# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements. See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License. You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

sudo: required
dist: trusty

language: python

matrix:
  include:
    # Spark 2.3.0
    - jdk: oraclejdk8
      python: 2.7
      env: >
        TEST_SPARK_VERSION=2.3.0

cache:
  directories:
  - $HOME/.sbt
  - $HOME/.m2

notifications:
  email: false

before_install:
  - export SPARK_HOME=$TRAVIS_BUILD_DIR/spark/spark-$TEST_SPARK_VERSION-bin-hadoop2.7

  # Download Python packages.
  - pip install coverage pycodestyle

  # Download Spark
  - mkdir $TRAVIS_BUILD_DIR/spark
  - curl -O http://mirrors.gigenet.com/apache/spark/spark-$TEST_SPARK_VERSION/spark-$TEST_SPARK_VERSION-bin-hadoop2.7.tgz
  - tar zxfC spark-$TEST_SPARK_VERSION-bin-hadoop2.7.tgz $TRAVIS_BUILD_DIR/spark

  # Set the PySpark libraries paths for, for example, py4j and pyspark. This should be set after Spark is downloaded.
  - export PYTHONPATH=$(ZIPS=("$SPARK_HOME"/python/lib/*.zip); IFS=:; echo "${ZIPS[*]}"):$PYTHONPATH

install:
  - build/sbt assembly -Drepourl=http://nexus-private.hortonworks.com/nexus/content/groups/public/ -Dhadoop.version=3.0.0.3.0.0.0-SNAPSHOT -Dhive.version=3.0.0.3.0.0.0-SNAPSHOT

  # sbt test:package generates a jar containing test classes complied. This is required to run the Python tests.
  - build/sbt test:package -Drepourl=http://nexus-private.hortonworks.com/nexus/content/groups/public/ -Dhadoop.version=3.0.0.3.0.0.0-SNAPSHOT -Dhive.version=3.0.0.3.0.0.0-SNAPSHOT

  # assembly also produces a Python API module under targets. Adds it to the Python path.
  - export PYTHONPATH=$(ZIPS=(`pwd`/target/*.zip); IFS=:; echo "${ZIPS[*]}"):$PYTHONPATH

script:
  # Scala / Java
  - build/sbt test -Drepourl=http://nexus-private.hortonworks.com/nexus/content/groups/public/ -Dhadoop.version=3.0.0.3.0.0.0-SNAPSHOT -Dhive.version=3.0.0.3.0.0.0-SNAPSHOT
  - build/scalastyle

  # Python
  - cd python
  - python setup.py sdist
  - coverage run pyspark_llap/sql/tests.py
  - coverage report --include "*pyspark_llap-*pyspark_llap/*" --show-missing
  - pycodestyle `find . -name '*.py'` --ignore=E402,E731,E241,W503,E226,E722,E741,E305 --max-line-length=100


diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
index 94244dd..bff325a 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
@@ -795,6 +795,27 @@ object SQLConf {
       .intConf
       .createWithDefault(UnsafeExternalSorter.DEFAULT_NUM_ELEMENTS_FOR_SPILL_THRESHOLD.toInt)
 
+  // ------------------------------------------------------
+  //  Configuration for HDP Ranger with LLAP
+  // ------------------------------------------------------
+  val HIVESERVER2_JDBC_URL =
+  buildConf("spark.sql.hive.hiveserver2.jdbc.url")
+    .doc("HiveServer2 JDBC URL.")
+    .stringConf
+    .createWithDefault("")
+
+  val HIVESERVER2_JDBC_URL_PRINCIPAL =
+    buildConf("spark.sql.hive.hiveserver2.jdbc.url.principal")
+      .doc("HiveServer2 JDBC Principal.")
+      .stringConf
+      .createWithDefault("")
+
+  val HIVESERVER2_CREDENTIAL_ENABLED =
+    buildConf("spark.yarn.security.credentials.hiveserver2.enabled")
+      .doc("When true, HiveServer2 credential provider is enabled.")
+      .booleanConf
+      .createWithDefault(false)
+
   object Deprecated {
     val MAPRED_REDUCE_TASKS = "mapred.reduce.tasks"
   }
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala b/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala
index ad5f7d6..4c88c5a 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala
@@ -1036,13 +1036,14 @@ object SparkSession {
 
   private val HIVE_SESSION_STATE_BUILDER_CLASS_NAME =
     "org.apache.spark.sql.hive.HiveSessionStateBuilder"
-  private val LLAP_SESSION_STATE_CLASS_NAME = "org.apache.spark.sql.hive.llap.LlapSessionState"
+  private val LLAP_SESSION_STATE_BUILDER_CLASS_NAME =
+    "org.apache.spark.sql.hive.llap.LlapSessionStateBuilder"
 
   private def sessionStateClassName(conf: SparkConf): String = {
     conf.get(CATALOG_IMPLEMENTATION) match {
       case "hive" =>
         if (isLLAPEnabled(conf)) {
-           LLAP_SESSION_STATE_CLASS_NAME
+          LLAP_SESSION_STATE_BUILDER_CLASS_NAME
         }
         else {
           HIVE_SESSION_STATE_BUILDER_CLASS_NAME
@@ -1090,7 +1091,7 @@ object SparkSession {
   private[spark] def isLLAPEnabled(conf: SparkConf): Boolean = {
     if (conf.get(LLAP_ENABLED.key, "false") == "true") {
       try {
-        Utils.classForName(LLAP_SESSION_STATE_CLASS_NAME)
+        Utils.classForName(LLAP_SESSION_STATE_BUILDER_CLASS_NAME)
         Utils.classForName("org.apache.hadoop.hive.conf.HiveConf")
         true
       } catch {
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala
index ded9303..be35ceb 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala
@@ -218,19 +218,18 @@ class FindDataSourceTable(sparkSession: SparkSession) extends Rule[LogicalPlan]
     val plan = catalogProxy.getCachedPlan(qualifiedTableName, new Callable[LogicalPlan]() {
       override def call(): LogicalPlan = {
         val pathOption = table.storage.locationUri.map("path" -> CatalogUtils.URIToString(_))
-        val dataSource =
-          DataSource(
-            sparkSession,
-            // In older version(prior to 2.1) of Spark, the table schema can be empty and should be
-            // inferred at runtime. We should still support it.
-            userSpecifiedSchema = if (table.schema.isEmpty) None else Some(table.schema),
-            partitionColumns = table.partitionColumnNames,
-            bucketSpec = table.bucketSpec,
-            className = table.provider.get,
-            options = table.storage.properties ++ pathOption,
-            catalogTable = Some(table))
-
-        LogicalRelation(dataSource.resolveRelation(checkFilesExist = false), table)
+          val dataSource =
+            DataSource(
+              sparkSession,
+              // In older version(prior to 2.1) of Spark, the table schema can be empty
+              // and should be inferred at runtime. We should still support it.
+              userSpecifiedSchema = if (table.schema.isEmpty) None else Some(table.schema),
+              partitionColumns = table.partitionColumnNames,
+              bucketSpec = table.bucketSpec,
+              className = table.provider.get,
+              options = table.storage.properties ++ pathOption,
+              catalogTable = Some(table))
+          LogicalRelation(dataSource.resolveRelation(checkFilesExist = false), table)
       }
     }).asInstanceOf[LogicalRelation]
 
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala b/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala
index 1b341a1..a6f2632 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala
@@ -98,6 +98,58 @@ private[sql] class SessionState(
   def refreshTable(tableName: String): Unit = {
     catalog.refreshTable(sqlParser.parseTableIdentifier(tableName))
   }
+
+  // ------------------------------------------------------
+  //  Helper methods for HDP Ranger with LLAP
+  // ------------------------------------------------------
+  /**
+   * @return User name for the STS connection
+   */
+  def getUserString(): String = {
+    System.getProperty("user.name")
+  }
+
+  /**
+   * Return connection URL (with replaced proxy user name if exists).
+   */
+  def getConnectionUrl(sparkSession: SparkSession): String = {
+    var userString = getUserString()
+    if (userString == null) {
+      userString = ""
+    }
+    val urlString = getConnectionUrlFromConf(sparkSession)
+    urlString.replace("${user}", userString)
+  }
+
+  import org.apache.spark.sql.internal.SQLConf._
+  /**
+   * For the given HiveServer2 JDBC URLs, attach the postfix strings if needed.
+   *
+   * For kerberized clusters,
+   *
+   * 1. YARN cluster mode: ";auth=delegationToken"
+   * 2. YARN client mode: ";principal=hive/_HOST@EXAMPLE.COM"
+   *
+   * Non-kerberied clusters,
+   * 3. Use the given URLs.
+   */
+  private def getConnectionUrlFromConf(sparkSession: SparkSession): String = {
+    if (!sparkSession.conf.contains(HIVESERVER2_JDBC_URL.key)) {
+      throw new Exception("Spark conf does not contain config " + HIVESERVER2_JDBC_URL.key)
+    }
+
+    if (sparkSession.conf.get(HIVESERVER2_CREDENTIAL_ENABLED, false)) {
+      // 1. YARN Cluster mode for kerberized clusters
+      s"${sparkSession.conf.get(HIVESERVER2_JDBC_URL.key)};auth=delegationToken"
+    } else if (sparkSession.sparkContext.conf.contains(HIVESERVER2_JDBC_URL_PRINCIPAL.key)) {
+      // 2. YARN Client mode for kerberized clusters
+      s"${sparkSession.conf.get(HIVESERVER2_JDBC_URL.key)};" +
+        s"principal=${sparkSession.conf.get(HIVESERVER2_JDBC_URL_PRINCIPAL.key)}"
+    } else {
+      // 3. For non-kerberized cluster
+      sparkSession.conf.get(HIVESERVER2_JDBC_URL.key)
+    }
+  }
 }
 
 private[sql] object SessionState {

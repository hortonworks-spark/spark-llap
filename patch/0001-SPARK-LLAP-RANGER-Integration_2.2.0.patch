From f7611c19388bde7bba3e2abbc75cf567d696fa68 Mon Sep 17 00:00:00 2001
From: Mingjie Tang <mtang@hortonworks.com>
Date: Mon, 8 May 2017 19:22:08 -0700
Subject: [PATCH 1/2] fix for compliation

---
 .../org/apache/spark/sql/internal/SQLConf.scala    | 21 +++++++++
 .../apache/spark/sql/internal/SessionState.scala   | 52 ++++++++++++++++++++++
 2 files changed, 73 insertions(+)

diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
index b24419a..e359160 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
@@ -787,6 +787,27 @@ object SQLConf {
       .intConf
       .createWithDefault(UnsafeExternalSorter.DEFAULT_NUM_ELEMENTS_FOR_SPILL_THRESHOLD.toInt)
 
+  // ------------------------------------------------------
+  //  Configuration for HDP Ranger with LLAP
+  // ------------------------------------------------------
+  val HIVESERVER2_JDBC_URL =
+  buildConf("spark.sql.hive.hiveserver2.jdbc.url")
+    .doc("HiveServer2 JDBC URL.")
+    .stringConf
+    .createWithDefault("")
+
+  val HIVESERVER2_JDBC_URL_PRINCIPAL =
+    buildConf("spark.sql.hive.hiveserver2.jdbc.url.principal")
+      .doc("HiveServer2 JDBC Principal.")
+      .stringConf
+      .createWithDefault("")
+
+  val HIVESERVER2_CREDENTIAL_ENABLED =
+    buildConf("spark.yarn.security.credentials.hiveserver2.enabled")
+      .doc("When true, HiveServer2 credential provider is enabled.")
+      .booleanConf
+      .createWithDefault(false)
+
   object Deprecated {
     val MAPRED_REDUCE_TASKS = "mapred.reduce.tasks"
   }
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala b/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala
index 1b341a1..a90c096 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala
@@ -98,6 +98,58 @@ private[sql] class SessionState(
   def refreshTable(tableName: String): Unit = {
     catalog.refreshTable(sqlParser.parseTableIdentifier(tableName))
   }
+
+  // ------------------------------------------------------
+  //  Helper methods for HDP Ranger with LLAP
+  // ------------------------------------------------------
+  /**
+   * @return User name for the STS connection
+   */
+  def getUserString(): String = {
+    System.getProperty("user")
+  }
+
+  /**
+   * Return connection URL (with replaced proxy user name if exists).
+   */
+  def getConnectionUrl(sparkSession: SparkSession): String = {
+    var userString = getUserString()
+    if (userString == null) {
+      userString = ""
+    }
+    val urlString = getConnectionUrlFromConf(sparkSession)
+    urlString.replace("${user}", userString)
+  }
+
+  import org.apache.spark.sql.internal.SQLConf._
+  /**
+   * For the given HiveServer2 JDBC URLs, attach the postfix strings if needed.
+   *
+   * For kerberized clusters,
+   *
+   * 1. YARN cluster mode: ";auth=delegationToken"
+   * 2. YARN client mode: ";principal=hive/_HOST@EXAMPLE.COM"
+   *
+   * Non-kerberied clusters,
+   * 3. Use the given URLs.
+   */
+  private def getConnectionUrlFromConf(sparkSession: SparkSession): String = {
+    if (!sparkSession.conf.contains(HIVESERVER2_JDBC_URL.key)) {
+      throw new Exception("Spark conf does not contain config " + HIVESERVER2_JDBC_URL.key)
+    }
+
+    if (sparkSession.conf.get(HIVESERVER2_CREDENTIAL_ENABLED, false)) {
+      // 1. YARN Cluster mode for kerberized clusters
+      s"${sparkSession.conf.get(HIVESERVER2_JDBC_URL.key)};auth=delegationToken"
+    } else if (sparkSession.sparkContext.conf.contains(HIVESERVER2_JDBC_URL_PRINCIPAL.key)) {
+      // 2. YARN Client mode for kerberized clusters
+      s"${sparkSession.conf.get(HIVESERVER2_JDBC_URL.key)};" +
+        s"principal=${sparkSession.conf.get(HIVESERVER2_JDBC_URL_PRINCIPAL.key)}"
+    } else {
+      // 3. For non-kerberized cluster
+      sparkSession.conf.get(HIVESERVER2_JDBC_URL.key)
+    }
+  }
 }
 
 private[sql] object SessionState {
-- 
2.10.1 (Apple Git-78)


From f4ab97c52a70057ea2524efd5e512f9177e2023d Mon Sep 17 00:00:00 2001
From: Mingjie Tang <mtang@hortonworks.com>
Date: Tue, 9 May 2017 15:21:00 -0700
Subject: [PATCH 2/2] update the datasource for find data source logical
 relation

---
 .../execution/datasources/DataSourceStrategy.scala | 38 ++++++++++++++--------
 1 file changed, 25 insertions(+), 13 deletions(-)

diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala
index d307122..3a748b5 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala
@@ -219,19 +219,31 @@ class FindDataSourceTable(sparkSession: SparkSession) extends Rule[LogicalPlan]
     val plan = cache.get(qualifiedTableName, new Callable[LogicalPlan]() {
       override def call(): LogicalPlan = {
         val pathOption = table.storage.locationUri.map("path" -> CatalogUtils.URIToString(_))
-        val dataSource =
-          DataSource(
-            sparkSession,
-            // In older version(prior to 2.1) of Spark, the table schema can be empty and should be
-            // inferred at runtime. We should still support it.
-            userSpecifiedSchema = if (table.schema.isEmpty) None else Some(table.schema),
-            partitionColumns = table.partitionColumnNames,
-            bucketSpec = table.bucketSpec,
-            className = table.provider.get,
-            options = table.storage.properties ++ pathOption,
-            catalogTable = Some(table))
-
-        LogicalRelation(dataSource.resolveRelation(checkFilesExist = false), table)
+
+        if(SparkSession.isLLAPEnabled(sparkSession.sparkContext.conf)) {
+          val llapDataSource =
+            DataSource(
+            sparkSession = sparkSession,
+            className = "org.apache.spark.sql.hive.llap",
+            options = Map(
+              "table" -> (qualifiedTableName.database + "." + qualifiedTableName.name),
+              "url" -> sparkSession.sessionState.getConnectionUrl(sparkSession)))
+            LogicalRelation(llapDataSource.resolveRelation(checkFilesExist = false), table)
+          } else {
+            val dataSource =
+              DataSource(
+                sparkSession,
+                // In older version(prior to 2.1) of Spark, the table schema can be empty
+                // and should be inferred at runtime. We should still support it.
+                userSpecifiedSchema = if (table.schema.isEmpty) None else Some(table.schema),
+                partitionColumns = table.partitionColumnNames,
+                bucketSpec = table.bucketSpec,
+                className = table.provider.get,
+                options = table.storage.properties ++ pathOption,
+                catalogTable = Some(table))
+            LogicalRelation(dataSource.resolveRelation(checkFilesExist = false), table)
+          }
+
       }
     }).asInstanceOf[LogicalRelation]
 
-- 
2.10.1 (Apple Git-78)


From 8de93e4e38576b996354a14ecec3e1e1e90ae849 Mon Sep 17 00:00:00 2001
From: Mingjie Tang <mtang@hortonworks.com>
Date: Tue, 2 May 2017 15:58:56 -0700
Subject: [PATCH 1/6] add the llap for the ranger

---
 .../security/ConfigurableCredentialManager.scala   |   2 +-
 .../security/HiveServer2CredentialProvider.scala   | 107 +++++++++++++++++++++
 .../apache/spark/sql/internal/StaticSQLConf.scala  |   6 ++
 .../scala/org/apache/spark/sql/SparkSession.scala  |  36 ++++++-
 .../apache/spark/sql/internal/SharedState.scala    |  10 +-
 5 files changed, 157 insertions(+), 4 deletions(-)
 create mode 100644 resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/security/HiveServer2CredentialProvider.scala

diff --git a/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/security/ConfigurableCredentialManager.scala b/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/security/ConfigurableCredentialManager.scala
index 4f4be52..6df415f 100644
--- a/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/security/ConfigurableCredentialManager.scala
+++ b/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/security/ConfigurableCredentialManager.scala
@@ -60,7 +60,7 @@ private[yarn] final class ConfigurableCredentialManager(
               s"using ${providerEnabledConfig.format(p.serviceName)} instead")
             c
           }
-        }.map(_.toBoolean).getOrElse(true)
+        }.map(_.toBoolean).getOrElse { if (p.serviceName == "hiveserver2") false else true }
     }.map { p => (p.serviceName, p) }.toMap
   }
 
diff --git a/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/security/HiveServer2CredentialProvider.scala b/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/security/HiveServer2CredentialProvider.scala
new file mode 100644
index 0000000..c8a4311
--- /dev/null
+++ b/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/security/HiveServer2CredentialProvider.scala
@@ -0,0 +1,107 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.deploy.yarn.security
+
+import java.lang.reflect.UndeclaredThrowableException
+import java.security.PrivilegedExceptionAction
+import java.sql.{Connection, DriverManager}
+
+import scala.reflect.runtime.universe
+
+import org.apache.hadoop.conf.Configuration
+import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier
+import org.apache.hadoop.io.Text
+import org.apache.hadoop.security.{Credentials, UserGroupInformation}
+import org.apache.hadoop.security.token.Token
+
+import org.apache.spark.SparkConf
+import org.apache.spark.internal.Logging
+import org.apache.spark.util.Utils
+
+private[security] class HiveServer2CredentialProvider extends ServiceCredentialProvider
+  with Logging {
+
+  override def serviceName: String = "hiveserver2"
+
+  override def obtainCredentials(
+                                  hadoopConf: Configuration,
+                                  sparkConf: SparkConf,
+                                  creds: Credentials): Option[Long] = {
+
+    var con: Connection = null
+    try {
+      Utils.classForName("org.apache.hive.jdbc.HiveDriver")
+
+      val mirror = universe.runtimeMirror(Utils.getContextOrSparkClassLoader)
+      val hiveConfClass = mirror.classLoader.loadClass("org.apache.hadoop.hive.conf.HiveConf")
+      val ctor = hiveConfClass.getDeclaredConstructor(classOf[Configuration],
+        classOf[Object].getClass)
+      val hiveConf = ctor.newInstance(hadoopConf, hiveConfClass).asInstanceOf[Configuration]
+
+      val hs2HostKey = "hive.server2.thrift.bind.host"
+      val hs2PortKey = "hive.server2.thrift.port"
+      val hs2PrincKey = "hive.server2.authentication.kerberos.principal"
+
+      require(hiveConf.get(hs2HostKey) != null, s"$hs2HostKey is not configured")
+      require(hiveConf.get(hs2PortKey) != null, s"$hs2PortKey is not configured")
+      require(hiveConf.get(hs2PrincKey) != null, s"$hs2PrincKey is not configured")
+
+      val jdbcUrl = s"jdbc:hive2://${hiveConf.get(hs2HostKey)}:${hiveConf.get(hs2PortKey)}/;" +
+        s"principal=${hiveConf.get(hs2PrincKey)}"
+
+      doAsRealUser {
+        con = DriverManager.getConnection(jdbcUrl)
+        val method = con.getClass.getMethod("getDelegationToken", classOf[String], classOf[String])
+        val currentUser = UserGroupInformation.getCurrentUser()
+        val realUser = Option(currentUser.getRealUser()).getOrElse(currentUser)
+        val tokenStr = method.invoke(con, realUser.getUserName, hiveConf.get(hs2PrincKey))
+          .asInstanceOf[String]
+        val token = new Token[DelegationTokenIdentifier]()
+        token.decodeFromUrlString(tokenStr)
+        creds.addToken(new Text("hive.jdbc.delegation.token"), token)
+        logInfo(s"Add HiveServer2 token $token to credentials")
+      }
+    } finally {
+      if (con != null) {
+        con.close()
+        con = null
+      }
+    }
+
+    None
+  }
+
+  /**
+    * Run some code as the real logged in user (which may differ from the current user, for
+    * example, when using proxying).
+    */
+  private def doAsRealUser[T](fn: => T): T = {
+    val currentUser = UserGroupInformation.getCurrentUser()
+    val realUser = Option(currentUser.getRealUser()).getOrElse(currentUser)
+
+    // For some reason the Scala-generated anonymous class ends up causing an
+    // UndeclaredThrowableException, even if you annotate the method with @throws.
+    try {
+      realUser.doAs(new PrivilegedExceptionAction[T]() {
+        override def run(): T = fn
+      })
+    } catch {
+      case e: UndeclaredThrowableException => throw Option(e.getCause()).getOrElse(e)
+    }
+  }
+}
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/StaticSQLConf.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/StaticSQLConf.scala
index c6c0a60..2ed4ae9 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/StaticSQLConf.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/StaticSQLConf.scala
@@ -44,6 +44,12 @@ object StaticSQLConf {
     .stringConf
     .createWithDefault("global_temp")
 
+  // Llap enabled configuration for HDP
+  val LLAP_ENABLED = buildStaticConf("spark.sql.hive.llap")
+    .internal()
+    .booleanConf
+    .createWithDefault(false)
+
   // This is used to control when we will split a schema's JSON string to multiple pieces
   // in order to fit the JSON string in metastore's table property (by default, the value has
   // a length restriction of 4000 characters, so do not use a value larger than 4000 as the default
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala b/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala
index a519492..76f71df 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala
@@ -39,7 +39,7 @@ import org.apache.spark.sql.execution._
 import org.apache.spark.sql.execution.datasources.LogicalRelation
 import org.apache.spark.sql.execution.ui.SQLListener
 import org.apache.spark.sql.internal._
-import org.apache.spark.sql.internal.StaticSQLConf.CATALOG_IMPLEMENTATION
+import org.apache.spark.sql.internal.StaticSQLConf.{CATALOG_IMPLEMENTATION, LLAP_ENABLED}
 import org.apache.spark.sql.sources.BaseRelation
 import org.apache.spark.sql.streaming._
 import org.apache.spark.sql.types.{DataType, StructType}
@@ -1031,9 +1031,18 @@ object SparkSession {
   private val HIVE_SESSION_STATE_BUILDER_CLASS_NAME =
     "org.apache.spark.sql.hive.HiveSessionStateBuilder"
 
+  // HDP Llap SessionStateBuilder
+  private val LLAP_SESSION_STATE_BUILDER_CLASS_NAME =
+  "org.apache.spark.sql.hive.LlapSessionStateBuilder"
+
   private def sessionStateClassName(conf: SparkConf): String = {
     conf.get(CATALOG_IMPLEMENTATION) match {
-      case "hive" => HIVE_SESSION_STATE_BUILDER_CLASS_NAME
+      case "hive" =>
+        if (isLLAPEnabled(conf)) {
+          LLAP_SESSION_STATE_BUILDER_CLASS_NAME
+        } else {
+          HIVE_SESSION_STATE_BUILDER_CLASS_NAME
+        }
       case "in-memory" => classOf[SessionStateBuilder].getCanonicalName
     }
   }
@@ -1069,4 +1078,27 @@ object SparkSession {
     }
   }
 
+
+  /**
+    * Return true if `spark.sql.hive.llap=true` and classes can be loaded.
+    * On class loading errors, it will fails.
+    * Return false if `spark.sql.hive.llap=false`.
+    */
+  private[spark] def isLLAPEnabled(conf: SparkConf): Boolean = {
+    if (conf.get(LLAP_ENABLED.key, "false") == "true") {
+      try {
+        Utils.classForName(LLAP_SESSION_STATE_BUILDER_CLASS_NAME)
+        Utils.classForName("org.apache.hadoop.hive.conf.HiveConf")
+        true
+      } catch {
+        case _: ClassNotFoundException | _: NoClassDefFoundError =>
+          throw new IllegalArgumentException(
+            "Unable to instantiate SparkSession with LLAP support because " +
+              "LLAP or Hive classes are not found.")
+      }
+    } else {
+      false
+    }
+  }
+
 }
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala b/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala
index a93b701..e0337ed 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala
@@ -166,9 +166,17 @@ object SharedState extends Logging {
 
   private val HIVE_EXTERNAL_CATALOG_CLASS_NAME = "org.apache.spark.sql.hive.HiveExternalCatalog"
 
+  private val LLAP_EXTERNAL_CATALOG_CLASS_NAME =
+    "org.apache.spark.sql.hive.llap.LlapExternalCatalog"
+
   private def externalCatalogClassName(conf: SparkConf): String = {
     conf.get(CATALOG_IMPLEMENTATION) match {
-      case "hive" => HIVE_EXTERNAL_CATALOG_CLASS_NAME
+      case "hive" =>
+        if (SparkSession.isLLAPEnabled(conf)) {
+          LLAP_EXTERNAL_CATALOG_CLASS_NAME
+        } else {
+          HIVE_EXTERNAL_CATALOG_CLASS_NAME
+        }
       case "in-memory" => classOf[InMemoryCatalog].getCanonicalName
     }
   }
-- 
2.10.1 (Apple Git-78)


From 2745d748b39f8cdabc5149c1c214473132b81f81 Mon Sep 17 00:00:00 2001
From: Mingjie Tang <mtang@hortonworks.com>
Date: Tue, 2 May 2017 18:42:23 -0700
Subject: [PATCH 2/6] fix the compile scala style type issue

---
 .../deploy/yarn/security/HiveServer2CredentialProvider.scala     | 6 +++---
 sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala  | 9 ++++-----
 2 files changed, 7 insertions(+), 8 deletions(-)

diff --git a/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/security/HiveServer2CredentialProvider.scala b/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/security/HiveServer2CredentialProvider.scala
index c8a4311..5774eca 100644
--- a/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/security/HiveServer2CredentialProvider.scala
+++ b/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/security/HiveServer2CredentialProvider.scala
@@ -87,9 +87,9 @@ private[security] class HiveServer2CredentialProvider extends ServiceCredentialP
   }
 
   /**
-    * Run some code as the real logged in user (which may differ from the current user, for
-    * example, when using proxying).
-    */
+   * Run some code as the real logged in user (which may differ from the current user, for
+   * example, when using proxying).
+   */
   private def doAsRealUser[T](fn: => T): T = {
     val currentUser = UserGroupInformation.getCurrentUser()
     val realUser = Option(currentUser.getRealUser()).getOrElse(currentUser)
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala b/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala
index 76f71df..aeb57b5 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala
@@ -1078,12 +1078,11 @@ object SparkSession {
     }
   }
 
-
   /**
-    * Return true if `spark.sql.hive.llap=true` and classes can be loaded.
-    * On class loading errors, it will fails.
-    * Return false if `spark.sql.hive.llap=false`.
-    */
+   * Return true if `spark.sql.hive.llap=true` and classes can be loaded.
+   * On class loading errors, it will fails.
+   * Return false if `spark.sql.hive.llap=false`.
+   */
   private[spark] def isLLAPEnabled(conf: SparkConf): Boolean = {
     if (conf.get(LLAP_ENABLED.key, "false") == "true") {
       try {
-- 
2.10.1 (Apple Git-78)


From f7611c19388bde7bba3e2abbc75cf567d696fa68 Mon Sep 17 00:00:00 2001
From: Mingjie Tang <mtang@hortonworks.com>
Date: Mon, 8 May 2017 19:22:08 -0700
Subject: [PATCH 3/6] fix for compliation

---
 .../org/apache/spark/sql/internal/SQLConf.scala    | 21 +++++++++
 .../apache/spark/sql/internal/SessionState.scala   | 52 ++++++++++++++++++++++
 2 files changed, 73 insertions(+)

diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
index b24419a..e359160 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
@@ -787,6 +787,27 @@ object SQLConf {
       .intConf
       .createWithDefault(UnsafeExternalSorter.DEFAULT_NUM_ELEMENTS_FOR_SPILL_THRESHOLD.toInt)
 
+  // ------------------------------------------------------
+  //  Configuration for HDP Ranger with LLAP
+  // ------------------------------------------------------
+  val HIVESERVER2_JDBC_URL =
+  buildConf("spark.sql.hive.hiveserver2.jdbc.url")
+    .doc("HiveServer2 JDBC URL.")
+    .stringConf
+    .createWithDefault("")
+
+  val HIVESERVER2_JDBC_URL_PRINCIPAL =
+    buildConf("spark.sql.hive.hiveserver2.jdbc.url.principal")
+      .doc("HiveServer2 JDBC Principal.")
+      .stringConf
+      .createWithDefault("")
+
+  val HIVESERVER2_CREDENTIAL_ENABLED =
+    buildConf("spark.yarn.security.credentials.hiveserver2.enabled")
+      .doc("When true, HiveServer2 credential provider is enabled.")
+      .booleanConf
+      .createWithDefault(false)
+
   object Deprecated {
     val MAPRED_REDUCE_TASKS = "mapred.reduce.tasks"
   }
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala b/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala
index 1b341a1..a90c096 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala
@@ -98,6 +98,58 @@ private[sql] class SessionState(
   def refreshTable(tableName: String): Unit = {
     catalog.refreshTable(sqlParser.parseTableIdentifier(tableName))
   }
+
+  // ------------------------------------------------------
+  //  Helper methods for HDP Ranger with LLAP
+  // ------------------------------------------------------
+  /**
+   * @return User name for the STS connection
+   */
+  def getUserString(): String = {
+    System.getProperty("user")
+  }
+
+  /**
+   * Return connection URL (with replaced proxy user name if exists).
+   */
+  def getConnectionUrl(sparkSession: SparkSession): String = {
+    var userString = getUserString()
+    if (userString == null) {
+      userString = ""
+    }
+    val urlString = getConnectionUrlFromConf(sparkSession)
+    urlString.replace("${user}", userString)
+  }
+
+  import org.apache.spark.sql.internal.SQLConf._
+  /**
+   * For the given HiveServer2 JDBC URLs, attach the postfix strings if needed.
+   *
+   * For kerberized clusters,
+   *
+   * 1. YARN cluster mode: ";auth=delegationToken"
+   * 2. YARN client mode: ";principal=hive/_HOST@EXAMPLE.COM"
+   *
+   * Non-kerberied clusters,
+   * 3. Use the given URLs.
+   */
+  private def getConnectionUrlFromConf(sparkSession: SparkSession): String = {
+    if (!sparkSession.conf.contains(HIVESERVER2_JDBC_URL.key)) {
+      throw new Exception("Spark conf does not contain config " + HIVESERVER2_JDBC_URL.key)
+    }
+
+    if (sparkSession.conf.get(HIVESERVER2_CREDENTIAL_ENABLED, false)) {
+      // 1. YARN Cluster mode for kerberized clusters
+      s"${sparkSession.conf.get(HIVESERVER2_JDBC_URL.key)};auth=delegationToken"
+    } else if (sparkSession.sparkContext.conf.contains(HIVESERVER2_JDBC_URL_PRINCIPAL.key)) {
+      // 2. YARN Client mode for kerberized clusters
+      s"${sparkSession.conf.get(HIVESERVER2_JDBC_URL.key)};" +
+        s"principal=${sparkSession.conf.get(HIVESERVER2_JDBC_URL_PRINCIPAL.key)}"
+    } else {
+      // 3. For non-kerberized cluster
+      sparkSession.conf.get(HIVESERVER2_JDBC_URL.key)
+    }
+  }
 }
 
 private[sql] object SessionState {
-- 
2.10.1 (Apple Git-78)


From f4ab97c52a70057ea2524efd5e512f9177e2023d Mon Sep 17 00:00:00 2001
From: Mingjie Tang <mtang@hortonworks.com>
Date: Tue, 9 May 2017 15:21:00 -0700
Subject: [PATCH 4/6] update the datasource for find data source logical
 relation

---
 .../execution/datasources/DataSourceStrategy.scala | 38 ++++++++++++++--------
 1 file changed, 25 insertions(+), 13 deletions(-)

diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala
index d307122..3a748b5 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala
@@ -219,19 +219,31 @@ class FindDataSourceTable(sparkSession: SparkSession) extends Rule[LogicalPlan]
     val plan = cache.get(qualifiedTableName, new Callable[LogicalPlan]() {
       override def call(): LogicalPlan = {
         val pathOption = table.storage.locationUri.map("path" -> CatalogUtils.URIToString(_))
-        val dataSource =
-          DataSource(
-            sparkSession,
-            // In older version(prior to 2.1) of Spark, the table schema can be empty and should be
-            // inferred at runtime. We should still support it.
-            userSpecifiedSchema = if (table.schema.isEmpty) None else Some(table.schema),
-            partitionColumns = table.partitionColumnNames,
-            bucketSpec = table.bucketSpec,
-            className = table.provider.get,
-            options = table.storage.properties ++ pathOption,
-            catalogTable = Some(table))
-
-        LogicalRelation(dataSource.resolveRelation(checkFilesExist = false), table)
+
+        if(SparkSession.isLLAPEnabled(sparkSession.sparkContext.conf)) {
+          val llapDataSource =
+            DataSource(
+            sparkSession = sparkSession,
+            className = "org.apache.spark.sql.hive.llap",
+            options = Map(
+              "table" -> (qualifiedTableName.database + "." + qualifiedTableName.name),
+              "url" -> sparkSession.sessionState.getConnectionUrl(sparkSession)))
+            LogicalRelation(llapDataSource.resolveRelation(checkFilesExist = false), table)
+          } else {
+            val dataSource =
+              DataSource(
+                sparkSession,
+                // In older version(prior to 2.1) of Spark, the table schema can be empty
+                // and should be inferred at runtime. We should still support it.
+                userSpecifiedSchema = if (table.schema.isEmpty) None else Some(table.schema),
+                partitionColumns = table.partitionColumnNames,
+                bucketSpec = table.bucketSpec,
+                className = table.provider.get,
+                options = table.storage.properties ++ pathOption,
+                catalogTable = Some(table))
+            LogicalRelation(dataSource.resolveRelation(checkFilesExist = false), table)
+          }
+
       }
     }).asInstanceOf[LogicalRelation]
 
-- 
2.10.1 (Apple Git-78)


From bd6ed375641214cd7963e8d1135a51eb7c3123f8 Mon Sep 17 00:00:00 2001
From: Mingjie Tang <mtang@hortonworks.com>
Date: Thu, 18 May 2017 20:03:38 -0700
Subject: [PATCH 5/6] update the class name

---
 sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala b/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala
index aeb57b5..53ed700 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala
@@ -1033,7 +1033,7 @@ object SparkSession {
 
   // HDP Llap SessionStateBuilder
   private val LLAP_SESSION_STATE_BUILDER_CLASS_NAME =
-  "org.apache.spark.sql.hive.LlapSessionStateBuilder"
+  "org.apache.spark.sql.hive.llap.LlapSessionStateBuilder"
 
   private def sessionStateClassName(conf: SparkConf): String = {
     conf.get(CATALOG_IMPLEMENTATION) match {
-- 
2.10.1 (Apple Git-78)


From 29bf2859b4fec9488c5651ef4ce73b32e4eea22a Mon Sep 17 00:00:00 2001
From: Mingjie Tang <mtang@hortonworks.com>
Date: Thu, 18 May 2017 23:09:09 -0700
Subject: [PATCH 6/6] update the for create table

---
 .../main/scala/org/apache/spark/sql/hive/HiveExternalCatalog.scala    | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveExternalCatalog.scala b/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveExternalCatalog.scala
index ba48fac..a730364 100644
--- a/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveExternalCatalog.scala
+++ b/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveExternalCatalog.scala
@@ -210,7 +210,7 @@ private[spark] class HiveExternalCatalog(conf: SparkConf, hadoopConf: Configurat
   // Tables
   // --------------------------------------------------------------------------
 
-  override protected def doCreateTable(
+  override def doCreateTable(
       tableDefinition: CatalogTable,
       ignoreIfExists: Boolean): Unit = withClient {
     assert(tableDefinition.identifier.database.isDefined)
@@ -473,7 +473,7 @@ private[spark] class HiveExternalCatalog(conf: SparkConf, hadoopConf: Configurat
     }
   }
 
-  override protected def doDropTable(
+  override def doDropTable(
       db: String,
       table: String,
       ignoreIfNotExists: Boolean,
-- 
2.10.1 (Apple Git-78)


From 65adfc925263f79366631ebac9b780a05031ff6c Mon Sep 17 00:00:00 2001
From: Dongjoon Hyun <dongjoon@apache.org>
Date: Tue, 24 Jan 2017 21:10:50 -0800
Subject: [PATCH] SPARK-LLAP-RANGER Integration

---
 .../scala/org/apache/spark/sql/SparkSession.scala  |  27 +++++-
 .../org/apache/spark/sql/internal/SQLConf.scala    |   5 +
 .../apache/spark/sql/internal/SharedState.scala    |  10 +-
 .../hive/thriftserver/SparkSQLSessionManager.scala |   1 +
 .../apache/spark/sql/hive/HiveSessionState.scala   |   9 ++
 .../execution/CreateHiveTableAsSelectCommand.scala |   7 +-
 ....deploy.yarn.security.ServiceCredentialProvider |   1 +
 .../security/ConfigurableCredentialManager.scala   |   2 +-
 .../security/HiveServer2CredentialProvider.scala   | 107 +++++++++++++++++++++
 9 files changed, 162 insertions(+), 7 deletions(-)
 create mode 100644 yarn/src/main/scala/org/apache/spark/deploy/yarn/security/HiveServer2CredentialProvider.scala

diff --git a/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala b/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala
index f3dde480ea..54e7240a3b 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala
@@ -41,7 +41,7 @@ import org.apache.spark.sql.execution._
 import org.apache.spark.sql.execution.datasources.LogicalRelation
 import org.apache.spark.sql.execution.ui.SQLListener
 import org.apache.spark.sql.internal.{CatalogImpl, SessionState, SharedState}
-import org.apache.spark.sql.internal.StaticSQLConf.CATALOG_IMPLEMENTATION
+import org.apache.spark.sql.internal.StaticSQLConf.{CATALOG_IMPLEMENTATION, LLAP_ENABLED}
 import org.apache.spark.sql.sources.BaseRelation
 import org.apache.spark.sql.streaming._
 import org.apache.spark.sql.types.{DataType, LongType, StructType}
@@ -957,10 +957,12 @@ object SparkSession {
   private val defaultSession = new AtomicReference[SparkSession]
 
   private val HIVE_SESSION_STATE_CLASS_NAME = "org.apache.spark.sql.hive.HiveSessionState"
+  private val LLAP_SESSION_STATE_CLASS_NAME = "org.apache.spark.sql.hive.llap.LlapSessionState"
 
   private def sessionStateClassName(conf: SparkConf): String = {
     conf.get(CATALOG_IMPLEMENTATION) match {
-      case "hive" => HIVE_SESSION_STATE_CLASS_NAME
+      case "hive" =>
+        if (isLLAPEnabled(conf)) LLAP_SESSION_STATE_CLASS_NAME else HIVE_SESSION_STATE_CLASS_NAME
       case "in-memory" => classOf[SessionState].getCanonicalName
     }
   }
@@ -995,4 +997,25 @@ object SparkSession {
     }
   }
 
+  /**
+   * Return true if `spark.sql.hive.llap=true` and classes can be loaded.
+   * On class loading errors, it will fails.
+   * Return false if `spark.sql.hive.llap=false`.
+   */
+  private[spark] def isLLAPEnabled(conf: SparkConf): Boolean = {
+    if (conf.get(LLAP_ENABLED.key, "false") == "true") {
+      try {
+        Utils.classForName(LLAP_SESSION_STATE_CLASS_NAME)
+        Utils.classForName("org.apache.hadoop.hive.conf.HiveConf")
+        true
+      } catch {
+        case _: ClassNotFoundException | _: NoClassDefFoundError =>
+          throw new IllegalArgumentException(
+            "Unable to instantiate SparkSession with LLAP support because " +
+              "LLAP or Hive classes are not found.")
+      }
+    } else {
+      false
+    }
+  }
 }
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala b/sql/core/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
index 5454be4c01..6bea7ab32b 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
@@ -942,6 +942,11 @@ object StaticSQLConf {
     .checkValues(Set("hive", "in-memory"))
     .createWithDefault("in-memory")
 
+  val LLAP_ENABLED = buildConf("spark.sql.hive.llap")
+    .internal()
+    .booleanConf
+    .createWithDefault(false)
+
   val GLOBAL_TEMP_DATABASE = buildConf("spark.sql.globalTempDatabase")
     .internal()
     .stringConf
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala b/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala
index 8de95fe64e..386ab90495 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala
@@ -141,10 +141,16 @@ private[sql] class SharedState(val sparkContext: SparkContext) extends Logging {
 object SharedState {
 
   private val HIVE_EXTERNAL_CATALOG_CLASS_NAME = "org.apache.spark.sql.hive.HiveExternalCatalog"
-
+  private val LLAP_EXTERNAL_CATALOG_CLASS_NAME =
+    "org.apache.spark.sql.hive.llap.LlapExternalCatalog"
   private def externalCatalogClassName(conf: SparkConf): String = {
     conf.get(CATALOG_IMPLEMENTATION) match {
-      case "hive" => HIVE_EXTERNAL_CATALOG_CLASS_NAME
+      case "hive" =>
+        if (SparkSession.isLLAPEnabled(conf)) {
+          LLAP_EXTERNAL_CATALOG_CLASS_NAME
+        } else {
+          HIVE_EXTERNAL_CATALOG_CLASS_NAME
+        }
       case "in-memory" => classOf[InMemoryCatalog].getCanonicalName
     }
   }
diff --git a/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLSessionManager.scala b/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLSessionManager.scala
index 226b7e175a..326cb8390a 100644
--- a/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLSessionManager.scala
+++ b/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLSessionManager.scala
@@ -78,6 +78,7 @@ private[hive] class SparkSQLSessionManager(hiveServer: HiveServer2, sqlContext:
     } else {
       sqlContext.newSession()
     }
+    ctx.sessionState.asInstanceOf[HiveSessionState].setUser(session.getUserName())
     ctx.setConf("spark.sql.hive.version", HiveUtils.hiveExecutionVersion)
     if (sessionConf != null && sessionConf.containsKey("use:database")) {
       ctx.sql(s"use ${sessionConf.get("use:database")}")
diff --git a/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveSessionState.scala b/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveSessionState.scala
index 6d4fe1a941..f286b0df18 100644
--- a/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveSessionState.scala
+++ b/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveSessionState.scala
@@ -147,4 +147,13 @@ private[hive] class HiveSessionState(sparkSession: SparkSession)
       "spark.sql.hive.thriftServer.singleSession", defaultValue = false)
   }
 
+  private var userName = System.getProperty("user.name")
+
+  def setUser(user: String): Unit = {
+    userName = user
+  }
+
+  def getUser(): String = {
+    userName
+  }
 }
diff --git a/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/CreateHiveTableAsSelectCommand.scala b/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/CreateHiveTableAsSelectCommand.scala
index cac43597ae..8ad83dac36 100644
--- a/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/CreateHiveTableAsSelectCommand.scala
+++ b/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/CreateHiveTableAsSelectCommand.scala
@@ -21,9 +21,11 @@ import scala.util.control.NonFatal
 
 import org.apache.spark.sql.{AnalysisException, Row, SparkSession}
 import org.apache.spark.sql.catalyst.catalog.CatalogTable
-import org.apache.spark.sql.catalyst.plans.logical.{InsertIntoTable, LogicalPlan, OverwriteOptions}
+import org.apache.spark.sql.catalyst.plans.logical._
 import org.apache.spark.sql.execution.command.RunnableCommand
+import org.apache.spark.sql.execution.datasources.LogicalRelation
 import org.apache.spark.sql.hive.MetastoreRelation
+import org.apache.spark.sql.sources.InsertableRelation
 
 
 /**
@@ -45,7 +47,7 @@ case class CreateHiveTableAsSelectCommand(
   override def innerChildren: Seq[LogicalPlan] = Seq(query)
 
   override def run(sparkSession: SparkSession): Seq[Row] = {
-    lazy val metastoreRelation: MetastoreRelation = {
+    lazy val metastoreRelation = {
       import org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
       import org.apache.hadoop.hive.serde2.`lazy`.LazySimpleSerDe
       import org.apache.hadoop.io.Text
@@ -74,6 +76,7 @@ case class CreateHiveTableAsSelectCommand(
       // Get the Metastore Relation
       sparkSession.sessionState.catalog.lookupRelation(tableIdentifier) match {
         case r: MetastoreRelation => r
+        case SubqueryAlias(_, r @ LogicalRelation(_: InsertableRelation, _, _), _) => r
       }
     }
     // TODO ideally, we should get the output data ready first and then
diff --git a/yarn/src/main/resources/META-INF/services/org.apache.spark.deploy.yarn.security.ServiceCredentialProvider b/yarn/src/main/resources/META-INF/services/org.apache.spark.deploy.yarn.security.ServiceCredentialProvider
index 22ead56d23..d63d5ce911 100644
--- a/yarn/src/main/resources/META-INF/services/org.apache.spark.deploy.yarn.security.ServiceCredentialProvider
+++ b/yarn/src/main/resources/META-INF/services/org.apache.spark.deploy.yarn.security.ServiceCredentialProvider
@@ -1,3 +1,4 @@
 org.apache.spark.deploy.yarn.security.HDFSCredentialProvider
 org.apache.spark.deploy.yarn.security.HBaseCredentialProvider
 org.apache.spark.deploy.yarn.security.HiveCredentialProvider
+org.apache.spark.deploy.yarn.security.HiveServer2CredentialProvider
diff --git a/yarn/src/main/scala/org/apache/spark/deploy/yarn/security/ConfigurableCredentialManager.scala b/yarn/src/main/scala/org/apache/spark/deploy/yarn/security/ConfigurableCredentialManager.scala
index c4c07b4930..b6d858a23a 100644
--- a/yarn/src/main/scala/org/apache/spark/deploy/yarn/security/ConfigurableCredentialManager.scala
+++ b/yarn/src/main/scala/org/apache/spark/deploy/yarn/security/ConfigurableCredentialManager.scala
@@ -58,7 +58,7 @@ private[yarn] final class ConfigurableCredentialManager(
               s"using ${providerEnabledConfig.format(p.serviceName)} instead")
             c
           }
-        }.map(_.toBoolean).getOrElse(true)
+        }.map(_.toBoolean).getOrElse { if (p.serviceName == "hiveserver2") false else true }
     }.map { p => (p.serviceName, p) }.toMap
   }
 
diff --git a/yarn/src/main/scala/org/apache/spark/deploy/yarn/security/HiveServer2CredentialProvider.scala b/yarn/src/main/scala/org/apache/spark/deploy/yarn/security/HiveServer2CredentialProvider.scala
new file mode 100644
index 0000000000..32802ddd56
--- /dev/null
+++ b/yarn/src/main/scala/org/apache/spark/deploy/yarn/security/HiveServer2CredentialProvider.scala
@@ -0,0 +1,107 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.deploy.yarn.security
+
+import java.lang.reflect.UndeclaredThrowableException
+import java.security.PrivilegedExceptionAction
+import java.sql.{Connection, DriverManager}
+
+import scala.reflect.runtime.universe
+import scala.util.control.NonFatal
+
+import org.apache.hadoop.conf.Configuration
+import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier
+import org.apache.hadoop.io.Text
+import org.apache.hadoop.security.{Credentials, UserGroupInformation}
+import org.apache.hadoop.security.token.Token
+
+import org.apache.spark.SparkConf
+import org.apache.spark.internal.Logging
+import org.apache.spark.util.Utils
+
+private[security] class HiveServer2CredentialProvider extends ServiceCredentialProvider
+    with Logging {
+
+  override def serviceName: String = "hiveserver2"
+
+  override def obtainCredentials(
+      hadoopConf: Configuration,
+      sparkConf: SparkConf,
+      creds: Credentials): Option[Long] = {
+
+    var con: Connection = null
+    try {
+      Utils.classForName("org.apache.hive.jdbc.HiveDriver")
+
+      val currentUser = UserGroupInformation.getCurrentUser()
+      val userName = if (sparkConf.getBoolean(
+          "spark.yarn.security.credentials.hiveserver2.useShortUserName", true)) {
+        currentUser.getShortUserName
+      } else {
+        currentUser.getUserName
+      }
+
+      val hs2Url = sparkConf.get("spark.sql.hive.hiveserver2.jdbc.url")
+      val principal = sparkConf.get("spark.sql.hive.hiveserver2.jdbc.url.principal")
+      require(hs2Url != null, "spark.sql.hive.hiveserver2.jdbc.url is not configured.")
+      require(principal != null,
+        "spark.sql.hive.hiveserver2.jdbc.url.principal is not configured.")
+
+      val jdbcUrl = s"$hs2Url;principal=$principal"
+      logInfo(s"Getting HS2 delegation token for $userName via $jdbcUrl")
+
+      doAsRealUser {
+        con = DriverManager.getConnection(jdbcUrl)
+        val method = con.getClass.getMethod("getDelegationToken", classOf[String], classOf[String])
+        val tokenStr = method.invoke(con, userName, principal).asInstanceOf[String]
+        val token = new Token[DelegationTokenIdentifier]()
+        token.decodeFromUrlString(tokenStr)
+        creds.addToken(new Text("hive.jdbc.delegation.token"), token)
+        logInfo(s"Added HS2 delegation token for $userName via $jdbcUrl")
+      }
+    } catch {
+      case NonFatal(e) => logWarning(s"Failed to get HS2 delegation token", e)
+    } finally {
+      if (con != null) {
+        con.close()
+        con = null
+      }
+    }
+
+    None
+  }
+
+  /**
+   * Run some code as the real logged in user (which may differ from the current user, for
+   * example, when using proxying).
+   */
+  private def doAsRealUser[T](fn: => T): T = {
+    val currentUser = UserGroupInformation.getCurrentUser()
+    val realUser = Option(currentUser.getRealUser()).getOrElse(currentUser)
+
+    // For some reason the Scala-generated anonymous class ends up causing an
+    // UndeclaredThrowableException, even if you annotate the method with @throws.
+    try {
+      realUser.doAs(new PrivilegedExceptionAction[T]() {
+        override def run(): T = fn
+      })
+    } catch {
+      case e: UndeclaredThrowableException => throw Option(e.getCause()).getOrElse(e)
+    }
+  }
+}
-- 
2.12.2

